{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5491c4c3",
   "metadata": {},
   "source": [
    "# Classification With Transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "5d4522b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "ad424941",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train = pd.read_csv(\"./data/aapl_5m_train.csv\").dropna()\n",
    "data_test = pd.read_csv(\"./data/aapl_5m_test.csv\").dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "4b24d839",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Timestamp</th>\n",
       "      <th>Gmtoffset</th>\n",
       "      <th>Datetime</th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Volume</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1609770600</td>\n",
       "      <td>0</td>\n",
       "      <td>2021-01-04 14:30:00</td>\n",
       "      <td>133.570007</td>\n",
       "      <td>133.611602</td>\n",
       "      <td>132.389999</td>\n",
       "      <td>132.809997</td>\n",
       "      <td>6624663.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1609770900</td>\n",
       "      <td>0</td>\n",
       "      <td>2021-01-04 14:35:00</td>\n",
       "      <td>132.750000</td>\n",
       "      <td>132.750000</td>\n",
       "      <td>131.809997</td>\n",
       "      <td>131.889999</td>\n",
       "      <td>2541553.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>1609771200</td>\n",
       "      <td>0</td>\n",
       "      <td>2021-01-04 14:40:00</td>\n",
       "      <td>131.500000</td>\n",
       "      <td>132.339996</td>\n",
       "      <td>131.500000</td>\n",
       "      <td>132.059997</td>\n",
       "      <td>2492415.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>1609771500</td>\n",
       "      <td>0</td>\n",
       "      <td>2021-01-04 14:45:00</td>\n",
       "      <td>132.000000</td>\n",
       "      <td>132.250000</td>\n",
       "      <td>131.899993</td>\n",
       "      <td>132.250000</td>\n",
       "      <td>1859131.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>1609771800</td>\n",
       "      <td>0</td>\n",
       "      <td>2021-01-04 14:50:00</td>\n",
       "      <td>132.000000</td>\n",
       "      <td>132.018096</td>\n",
       "      <td>131.520004</td>\n",
       "      <td>131.589996</td>\n",
       "      <td>1780105.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0   Timestamp  Gmtoffset             Datetime        Open  \\\n",
       "0           0  1609770600          0  2021-01-04 14:30:00  133.570007   \n",
       "1           1  1609770900          0  2021-01-04 14:35:00  132.750000   \n",
       "2           2  1609771200          0  2021-01-04 14:40:00  131.500000   \n",
       "3           3  1609771500          0  2021-01-04 14:45:00  132.000000   \n",
       "4           4  1609771800          0  2021-01-04 14:50:00  132.000000   \n",
       "\n",
       "         High         Low       Close     Volume  \n",
       "0  133.611602  132.389999  132.809997  6624663.0  \n",
       "1  132.750000  131.809997  131.889999  2541553.0  \n",
       "2  132.339996  131.500000  132.059997  2492415.0  \n",
       "3  132.250000  131.899993  132.250000  1859131.0  \n",
       "4  132.018096  131.520004  131.589996  1780105.0  "
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "900ac8c2",
   "metadata": {},
   "source": [
    "## Normalizing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "45cc2eec",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_mean = data_train.loc[:, [\"Open\", \"High\", \"Low\", \"Close\"]].mean()\n",
    "train_std = data_train.loc[:, [\"Open\", \"High\", \"Low\", \"Close\"]].std()\n",
    "\n",
    "norm_data_train = (data_train.loc[:, [\"Open\", \"High\", \"Low\", \"Close\"]] - train_mean) / train_std\n",
    "norm_data_test = (data_test.loc[:, [\"Open\", \"High\", \"Low\", \"Close\"]] - train_mean) / train_std"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78de9f2a",
   "metadata": {},
   "source": [
    "## Generating our features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "d2793a80",
   "metadata": {},
   "outputs": [],
   "source": [
    "lags = 5\n",
    "\n",
    "X_train = pd.DataFrame()\n",
    "X_test = pd.DataFrame()\n",
    "\n",
    "for lag in range(lags):\n",
    "    X_train[f\"Open_{lag}\"] = norm_data_train.Open.shift(lag)\n",
    "    X_train[f\"High_{lag}\"] = norm_data_train.High.shift(lag)\n",
    "    X_train[f\"Low_{lag}\"] = norm_data_train.Low.shift(lag)\n",
    "    X_train[f\"Close_{lag}\"] = norm_data_train.Close.shift(lag)\n",
    "    \n",
    "    X_test[f\"Open_{lag}\"] = norm_data_test.Open.shift(lag)\n",
    "    X_test[f\"High_{lag}\"] = norm_data_test.High.shift(lag)\n",
    "    X_test[f\"Low_{lag}\"] = norm_data_test.Low.shift(lag)\n",
    "    X_test[f\"Close_{lag}\"] = norm_data_test.Close.shift(lag)\n",
    "\n",
    "Y_train = (X_train.Close_0 * (1 + 0.01) < X_train.Close_0.shift(-1)).astype(float)\n",
    "Y_test = (X_test.Close_0 * (1 + 0.01) < X_test.Close_0.shift(-1)).astype(float)\n",
    "\n",
    "# Removing nans and last value\n",
    "X_train = X_train.iloc[5:-1, :].values\n",
    "X_test = X_test.iloc[5:-1, :].values\n",
    "\n",
    "Y_train = Y_train.iloc[5:-1].values.reshape(-1, 1)\n",
    "Y_test = Y_test.iloc[5:-1].values.reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "857138b6",
   "metadata": {},
   "source": [
    "## Reshaping Tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "4f687f9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = X_train.shape[1]\n",
    "\n",
    "X_train = X_train.reshape(-1, features, 1)\n",
    "X_test = X_test.reshape(-1, features, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60df0232",
   "metadata": {},
   "source": [
    "## Regular DNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "49a3596c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dnn = tf.keras.models.Sequential(layers=[\n",
    "    tf.keras.layers.Dense(units=120, activation=\"relu\", input_shape=(features, 1)),\n",
    "    tf.keras.layers.Dense(units=60, activation=\"relu\"),\n",
    "    tf.keras.layers.Dense(units=60, activation=\"relu\"),\n",
    "    tf.keras.layers.Dense(units=30, activation=\"relu\"),\n",
    "    tf.keras.layers.Dense(units=1, activation=\"softmax\")\n",
    "])\n",
    "# metric = tf.keras.metrics.SparseCategoricalAccuracy()\n",
    "\n",
    "dnn.compile(optimizer=\"adam\",\n",
    "            loss=\"sparse_categorical_crossentropy\",\n",
    "            metrics=[\"sparse_categorical_accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "1f98c0d4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_50 (Dense)            (None, 20, 120)           240       \n",
      "                                                                 \n",
      " dense_51 (Dense)            (None, 20, 60)            7260      \n",
      "                                                                 \n",
      " dense_52 (Dense)            (None, 20, 60)            3660      \n",
      "                                                                 \n",
      " dense_53 (Dense)            (None, 20, 30)            1830      \n",
      "                                                                 \n",
      " dense_54 (Dense)            (None, 20, 1)             31        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 13021 (50.86 KB)\n",
      "Trainable params: 13021 (50.86 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "dnn.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "35a45b73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "612/612 [==============================] - 1s 2ms/step - loss: 2.9957 - sparse_categorical_accuracy: 0.4722\n",
      "Epoch 2/20\n",
      "612/612 [==============================] - 1s 2ms/step - loss: 2.9957 - sparse_categorical_accuracy: 0.4722\n",
      "Epoch 3/20\n",
      "612/612 [==============================] - 1s 2ms/step - loss: 2.9957 - sparse_categorical_accuracy: 0.4722\n",
      "Epoch 4/20\n",
      "612/612 [==============================] - 1s 2ms/step - loss: 2.9957 - sparse_categorical_accuracy: 0.4722\n",
      "Epoch 5/20\n",
      "612/612 [==============================] - 1s 2ms/step - loss: 2.9957 - sparse_categorical_accuracy: 0.4722\n",
      "Epoch 6/20\n",
      "612/612 [==============================] - 1s 2ms/step - loss: 2.9957 - sparse_categorical_accuracy: 0.4722\n",
      "Epoch 7/20\n",
      "612/612 [==============================] - 1s 2ms/step - loss: 2.9957 - sparse_categorical_accuracy: 0.4722\n",
      "Epoch 8/20\n",
      "612/612 [==============================] - 1s 2ms/step - loss: 2.9957 - sparse_categorical_accuracy: 0.4722\n",
      "Epoch 9/20\n",
      "612/612 [==============================] - 1s 2ms/step - loss: 2.9957 - sparse_categorical_accuracy: 0.4722\n",
      "Epoch 10/20\n",
      "612/612 [==============================] - 1s 2ms/step - loss: 2.9957 - sparse_categorical_accuracy: 0.4722\n",
      "Epoch 11/20\n",
      "612/612 [==============================] - 1s 2ms/step - loss: 2.9957 - sparse_categorical_accuracy: 0.4722\n",
      "Epoch 12/20\n",
      "612/612 [==============================] - 1s 2ms/step - loss: 2.9957 - sparse_categorical_accuracy: 0.4722\n",
      "Epoch 13/20\n",
      "612/612 [==============================] - 1s 2ms/step - loss: 2.9957 - sparse_categorical_accuracy: 0.4722\n",
      "Epoch 14/20\n",
      "612/612 [==============================] - 1s 2ms/step - loss: 2.9957 - sparse_categorical_accuracy: 0.4722\n",
      "Epoch 15/20\n",
      "612/612 [==============================] - 1s 2ms/step - loss: 2.9957 - sparse_categorical_accuracy: 0.4722\n",
      "Epoch 16/20\n",
      "612/612 [==============================] - 1s 2ms/step - loss: 2.9957 - sparse_categorical_accuracy: 0.4722\n",
      "Epoch 17/20\n",
      "612/612 [==============================] - 1s 2ms/step - loss: 2.9957 - sparse_categorical_accuracy: 0.4722\n",
      "Epoch 18/20\n",
      "612/612 [==============================] - 1s 2ms/step - loss: 2.9957 - sparse_categorical_accuracy: 0.4722\n",
      "Epoch 19/20\n",
      "612/612 [==============================] - 1s 2ms/step - loss: 2.9957 - sparse_categorical_accuracy: 0.4722\n",
      "Epoch 20/20\n",
      "612/612 [==============================] - 1s 2ms/step - loss: 2.9957 - sparse_categorical_accuracy: 0.4722\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x2a3c95bd0>"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dnn.fit(X_train, Y_train, epochs=20, batch_size=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "053c8a0a",
   "metadata": {},
   "source": [
    "### Another way of creating a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "227f68ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "l1 = tf.keras.layers.Input(shape=(features, 1))\n",
    "l2 = tf.keras.layers.Dense(units=120, activation=\"relu\")(l1)\n",
    "l3 = tf.keras.layers.Dense(units=60, activation=\"relu\")(l2)\n",
    "l4 = tf.keras.layers.Dense(units=30, activation=\"relu\")(l3)\n",
    "\n",
    "outputs = tf.keras.layers.Dense(units=1, activation=\"softmax\")(l4)\n",
    "\n",
    "dnn2 = tf.keras.Model(inputs=l1, outputs=outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "bfbb0ff0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_11 (InputLayer)       [(None, 20, 1)]           0         \n",
      "                                                                 \n",
      " dense_59 (Dense)            (None, 20, 120)           240       \n",
      "                                                                 \n",
      " dense_60 (Dense)            (None, 20, 60)            7260      \n",
      "                                                                 \n",
      " dense_61 (Dense)            (None, 20, 30)            1830      \n",
      "                                                                 \n",
      " dense_62 (Dense)            (None, 20, 1)             31        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 9361 (36.57 KB)\n",
      "Trainable params: 9361 (36.57 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "dnn2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "ac1c326b",
   "metadata": {},
   "outputs": [],
   "source": [
    "dnn2.compile(optimizer=\"adam\",\n",
    "             loss=\"sparse_categorical_crossentropy\",\n",
    "             metrics=[\"sparse_categorical_accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "0b40ad35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1224/1224 [==============================] - 2s 1ms/step - loss: 2.9958 - sparse_categorical_accuracy: 0.4722\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x2a430eb50>"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dnn2.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9ab46b2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f923bf12",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "811c26aa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "deb6b2c1",
   "metadata": {},
   "source": [
    "## Classification Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "b0f2030a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_transformer(inputs, head_size, num_heads, dnn_dim):\n",
    "    # Stacking layers\n",
    "    l1 = tf.keras.layers.MultiHeadAttention(key_dim=head_size,\n",
    "                                            num_heads=num_heads,\n",
    "                                            dropout=0.2)(inputs, inputs)\n",
    "    l2 = tf.keras.layers.Dropout(0.2)(l1)\n",
    "    l3 = tf.keras.layers.LayerNormalization(epsilon=1e-6)(l2)\n",
    "    \n",
    "    res = l3 + inputs\n",
    "    \n",
    "    # Traditional DNN\n",
    "    l4 = tf.keras.layers.Conv1D(filters=4, kernel_size=1, activation=\"relu\")(res)\n",
    "    l5 = tf.keras.layers.Dropout(0.2)(l4)\n",
    "    l6 = tf.keras.layers.Conv1D(filters=inputs.shape[-1], kernel_size=1)(l5)\n",
    "    l7 = tf.keras.layers.LayerNormalization(epsilon=1e-6)(l6)\n",
    "    return l7 + res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "6af8c3f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = X_train.shape[1:]\n",
    "\n",
    "# Hyperparams\n",
    "head_size = 256\n",
    "num_heads = 4\n",
    "num_transformer_blocks = 4\n",
    "dnn_dim = 4\n",
    "units = 128\n",
    "\n",
    "\n",
    "# Defining input_shape as Input layer\n",
    "input_layer = tf.keras.layers.Input(input_shape)\n",
    "\n",
    "# Creating our transformers based on the input layer\n",
    "transformer_layers = input_layer\n",
    "\n",
    "for _ in range(num_transformer_blocks):\n",
    "    # Stacking transformers\n",
    "    transformer_layers = create_transformer(inputs=transformer_layers,\n",
    "                                            head_size=head_size,\n",
    "                                            num_heads=num_heads,\n",
    "                                            dnn_dim=dnn_dim)\n",
    "\n",
    "# Adding global pooling\n",
    "pooling_layer = tf.keras.layers.GlobalAveragePooling1D(data_format=\"channels_last\")\\\n",
    "                                                      (transformer_layers)\n",
    "\n",
    "# Adding MLP layers\n",
    "l1 = tf.keras.layers.Dense(units=128, activation=\"leaky_relu\")(pooling_layer)\n",
    "l2 = tf.keras.layers.Dropout(0.3)(l1)\n",
    "l3 = tf.keras.layers.Dense(units=128, activation=\"leaky_relu\")(l2)\n",
    "\n",
    "# Last layer, units = 2 for True and False values\n",
    "outputs = tf.keras.layers.Dense(units=2, activation=\"softmax\")(l3)\n",
    "\n",
    "# Model\n",
    "model = tf.keras.Model(inputs=input_layer,\n",
    "                       outputs=outputs,\n",
    "                       name=\"transformers_classification\")\n",
    "\n",
    "metric = tf.keras.metrics.SparseCategoricalAccuracy()\n",
    "adam_optimizer = tf.keras.optimizers.legacy.Adam(learning_rate=1e-4)\n",
    "#callbacks = [tf.keras.callbacks.EarlyStopping(monitor=\"loss\",\n",
    "#                                              patience=10,\n",
    "#                                              restore_best_weights=True)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "247c9e7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    loss=\"sparse_categorical_crossentropy\",\n",
    "    optimizer=adam_optimizer,\n",
    "    metrics=[metric],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "11bb281c",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"transformers_classification\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " input_9 (InputLayer)        [(None, 20, 1)]              0         []                            \n",
      "                                                                                                  \n",
      " multi_head_attention_32 (M  (None, 20, 1)                7169      ['input_9[0][0]',             \n",
      " ultiHeadAttention)                                                  'input_9[0][0]']             \n",
      "                                                                                                  \n",
      " dropout_72 (Dropout)        (None, 20, 1)                0         ['multi_head_attention_32[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " layer_normalization_64 (La  (None, 20, 1)                2         ['dropout_72[0][0]']          \n",
      " yerNormalization)                                                                                \n",
      "                                                                                                  \n",
      " tf.__operators__.add_64 (T  (None, 20, 1)                0         ['layer_normalization_64[0][0]\n",
      " FOpLambda)                                                         ',                            \n",
      "                                                                     'input_9[0][0]']             \n",
      "                                                                                                  \n",
      " conv1d_64 (Conv1D)          (None, 20, 4)                8         ['tf.__operators__.add_64[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " dropout_73 (Dropout)        (None, 20, 4)                0         ['conv1d_64[0][0]']           \n",
      "                                                                                                  \n",
      " conv1d_65 (Conv1D)          (None, 20, 1)                5         ['dropout_73[0][0]']          \n",
      "                                                                                                  \n",
      " layer_normalization_65 (La  (None, 20, 1)                2         ['conv1d_65[0][0]']           \n",
      " yerNormalization)                                                                                \n",
      "                                                                                                  \n",
      " tf.__operators__.add_65 (T  (None, 20, 1)                0         ['layer_normalization_65[0][0]\n",
      " FOpLambda)                                                         ',                            \n",
      "                                                                     'tf.__operators__.add_64[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " multi_head_attention_33 (M  (None, 20, 1)                7169      ['tf.__operators__.add_65[0][0\n",
      " ultiHeadAttention)                                                 ]',                           \n",
      "                                                                     'tf.__operators__.add_65[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " dropout_74 (Dropout)        (None, 20, 1)                0         ['multi_head_attention_33[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " layer_normalization_66 (La  (None, 20, 1)                2         ['dropout_74[0][0]']          \n",
      " yerNormalization)                                                                                \n",
      "                                                                                                  \n",
      " tf.__operators__.add_66 (T  (None, 20, 1)                0         ['layer_normalization_66[0][0]\n",
      " FOpLambda)                                                         ',                            \n",
      "                                                                     'tf.__operators__.add_65[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " conv1d_66 (Conv1D)          (None, 20, 4)                8         ['tf.__operators__.add_66[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " dropout_75 (Dropout)        (None, 20, 4)                0         ['conv1d_66[0][0]']           \n",
      "                                                                                                  \n",
      " conv1d_67 (Conv1D)          (None, 20, 1)                5         ['dropout_75[0][0]']          \n",
      "                                                                                                  \n",
      " layer_normalization_67 (La  (None, 20, 1)                2         ['conv1d_67[0][0]']           \n",
      " yerNormalization)                                                                                \n",
      "                                                                                                  \n",
      " tf.__operators__.add_67 (T  (None, 20, 1)                0         ['layer_normalization_67[0][0]\n",
      " FOpLambda)                                                         ',                            \n",
      "                                                                     'tf.__operators__.add_66[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " multi_head_attention_34 (M  (None, 20, 1)                7169      ['tf.__operators__.add_67[0][0\n",
      " ultiHeadAttention)                                                 ]',                           \n",
      "                                                                     'tf.__operators__.add_67[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " dropout_76 (Dropout)        (None, 20, 1)                0         ['multi_head_attention_34[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " layer_normalization_68 (La  (None, 20, 1)                2         ['dropout_76[0][0]']          \n",
      " yerNormalization)                                                                                \n",
      "                                                                                                  \n",
      " tf.__operators__.add_68 (T  (None, 20, 1)                0         ['layer_normalization_68[0][0]\n",
      " FOpLambda)                                                         ',                            \n",
      "                                                                     'tf.__operators__.add_67[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " conv1d_68 (Conv1D)          (None, 20, 4)                8         ['tf.__operators__.add_68[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " dropout_77 (Dropout)        (None, 20, 4)                0         ['conv1d_68[0][0]']           \n",
      "                                                                                                  \n",
      " conv1d_69 (Conv1D)          (None, 20, 1)                5         ['dropout_77[0][0]']          \n",
      "                                                                                                  \n",
      " layer_normalization_69 (La  (None, 20, 1)                2         ['conv1d_69[0][0]']           \n",
      " yerNormalization)                                                                                \n",
      "                                                                                                  \n",
      " tf.__operators__.add_69 (T  (None, 20, 1)                0         ['layer_normalization_69[0][0]\n",
      " FOpLambda)                                                         ',                            \n",
      "                                                                     'tf.__operators__.add_68[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " multi_head_attention_35 (M  (None, 20, 1)                7169      ['tf.__operators__.add_69[0][0\n",
      " ultiHeadAttention)                                                 ]',                           \n",
      "                                                                     'tf.__operators__.add_69[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " dropout_78 (Dropout)        (None, 20, 1)                0         ['multi_head_attention_35[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " layer_normalization_70 (La  (None, 20, 1)                2         ['dropout_78[0][0]']          \n",
      " yerNormalization)                                                                                \n",
      "                                                                                                  \n",
      " tf.__operators__.add_70 (T  (None, 20, 1)                0         ['layer_normalization_70[0][0]\n",
      " FOpLambda)                                                         ',                            \n",
      "                                                                     'tf.__operators__.add_69[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " conv1d_70 (Conv1D)          (None, 20, 4)                8         ['tf.__operators__.add_70[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " dropout_79 (Dropout)        (None, 20, 4)                0         ['conv1d_70[0][0]']           \n",
      "                                                                                                  \n",
      " conv1d_71 (Conv1D)          (None, 20, 1)                5         ['dropout_79[0][0]']          \n",
      "                                                                                                  \n",
      " layer_normalization_71 (La  (None, 20, 1)                2         ['conv1d_71[0][0]']           \n",
      " yerNormalization)                                                                                \n",
      "                                                                                                  \n",
      " tf.__operators__.add_71 (T  (None, 20, 1)                0         ['layer_normalization_71[0][0]\n",
      " FOpLambda)                                                         ',                            \n",
      "                                                                     'tf.__operators__.add_70[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " global_average_pooling1d_8  (None, 1)                    0         ['tf.__operators__.add_71[0][0\n",
      "  (GlobalAveragePooling1D)                                          ]']                           \n",
      "                                                                                                  \n",
      " dense_24 (Dense)            (None, 128)                  256       ['global_average_pooling1d_8[0\n",
      "                                                                    ][0]']                        \n",
      "                                                                                                  \n",
      " dropout_80 (Dropout)        (None, 128)                  0         ['dense_24[0][0]']            \n",
      "                                                                                                  \n",
      " dense_25 (Dense)            (None, 128)                  16512     ['dropout_80[0][0]']          \n",
      "                                                                                                  \n",
      " dense_26 (Dense)            (None, 2)                    258       ['dense_25[0][0]']            \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 45770 (178.79 KB)\n",
      "Trainable params: 45770 (178.79 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "fdcfc8c6",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "612/612 [==============================] - 42s 66ms/step - loss: 0.5604 - sparse_categorical_accuracy: 0.7089\n",
      "Epoch 2/100\n",
      "612/612 [==============================] - 41s 67ms/step - loss: 0.5457 - sparse_categorical_accuracy: 0.7188\n",
      "Epoch 3/100\n",
      "612/612 [==============================] - 41s 66ms/step - loss: 0.5458 - sparse_categorical_accuracy: 0.7185\n",
      "Epoch 4/100\n",
      "612/612 [==============================] - 41s 67ms/step - loss: 0.5455 - sparse_categorical_accuracy: 0.7190\n",
      "Epoch 5/100\n",
      "612/612 [==============================] - 41s 66ms/step - loss: 0.5452 - sparse_categorical_accuracy: 0.7180\n",
      "Epoch 6/100\n",
      "612/612 [==============================] - 40s 66ms/step - loss: 0.5450 - sparse_categorical_accuracy: 0.7186\n",
      "Epoch 7/100\n",
      "612/612 [==============================] - 40s 66ms/step - loss: 0.5449 - sparse_categorical_accuracy: 0.7184\n",
      "Epoch 8/100\n",
      "612/612 [==============================] - 41s 67ms/step - loss: 0.5451 - sparse_categorical_accuracy: 0.7186\n",
      "Epoch 9/100\n",
      "612/612 [==============================] - 41s 67ms/step - loss: 0.5453 - sparse_categorical_accuracy: 0.7183\n",
      "Epoch 10/100\n",
      "612/612 [==============================] - 41s 67ms/step - loss: 0.5454 - sparse_categorical_accuracy: 0.7185\n",
      "Epoch 11/100\n",
      "612/612 [==============================] - 41s 67ms/step - loss: 0.5450 - sparse_categorical_accuracy: 0.7179\n",
      "Epoch 12/100\n",
      "612/612 [==============================] - 41s 67ms/step - loss: 0.5451 - sparse_categorical_accuracy: 0.7185\n",
      "Epoch 13/100\n",
      "612/612 [==============================] - 40s 66ms/step - loss: 0.5448 - sparse_categorical_accuracy: 0.7179\n",
      "Epoch 14/100\n",
      "612/612 [==============================] - 40s 66ms/step - loss: 0.5450 - sparse_categorical_accuracy: 0.7182\n",
      "Epoch 15/100\n",
      "612/612 [==============================] - 40s 66ms/step - loss: 0.5449 - sparse_categorical_accuracy: 0.7187\n",
      "Epoch 16/100\n",
      "612/612 [==============================] - 41s 67ms/step - loss: 0.5450 - sparse_categorical_accuracy: 0.7178\n",
      "Epoch 17/100\n",
      "612/612 [==============================] - 41s 66ms/step - loss: 0.5450 - sparse_categorical_accuracy: 0.7187\n",
      "Epoch 18/100\n",
      "612/612 [==============================] - 41s 66ms/step - loss: 0.5447 - sparse_categorical_accuracy: 0.7183\n",
      "Epoch 19/100\n",
      "612/612 [==============================] - 41s 66ms/step - loss: 0.5451 - sparse_categorical_accuracy: 0.7182\n",
      "Epoch 20/100\n",
      "612/612 [==============================] - 41s 67ms/step - loss: 0.5448 - sparse_categorical_accuracy: 0.7185\n",
      "Epoch 21/100\n",
      "612/612 [==============================] - 41s 67ms/step - loss: 0.5446 - sparse_categorical_accuracy: 0.7187\n",
      "Epoch 22/100\n",
      "612/612 [==============================] - 41s 66ms/step - loss: 0.5449 - sparse_categorical_accuracy: 0.7186\n",
      "Epoch 23/100\n",
      "612/612 [==============================] - 40s 66ms/step - loss: 0.5448 - sparse_categorical_accuracy: 0.7186\n",
      "Epoch 24/100\n",
      "612/612 [==============================] - 40s 66ms/step - loss: 0.5449 - sparse_categorical_accuracy: 0.7191\n",
      "Epoch 25/100\n",
      "612/612 [==============================] - 41s 66ms/step - loss: 0.5448 - sparse_categorical_accuracy: 0.7188\n",
      "Epoch 26/100\n",
      "612/612 [==============================] - 41s 66ms/step - loss: 0.5449 - sparse_categorical_accuracy: 0.7184\n",
      "Epoch 27/100\n",
      "612/612 [==============================] - 41s 67ms/step - loss: 0.5448 - sparse_categorical_accuracy: 0.7191\n",
      "Epoch 28/100\n",
      "612/612 [==============================] - 41s 67ms/step - loss: 0.5443 - sparse_categorical_accuracy: 0.7182\n",
      "Epoch 29/100\n",
      "612/612 [==============================] - 41s 67ms/step - loss: 0.5448 - sparse_categorical_accuracy: 0.7185\n",
      "Epoch 30/100\n",
      "612/612 [==============================] - 41s 68ms/step - loss: 0.5447 - sparse_categorical_accuracy: 0.7186\n",
      "Epoch 31/100\n",
      "612/612 [==============================] - 41s 67ms/step - loss: 0.5447 - sparse_categorical_accuracy: 0.7186\n",
      "Epoch 32/100\n",
      "612/612 [==============================] - 41s 67ms/step - loss: 0.5444 - sparse_categorical_accuracy: 0.7184\n",
      "Epoch 33/100\n",
      "612/612 [==============================] - 41s 67ms/step - loss: 0.5447 - sparse_categorical_accuracy: 0.7182\n",
      "Epoch 34/100\n",
      "612/612 [==============================] - 42s 68ms/step - loss: 0.5445 - sparse_categorical_accuracy: 0.7180\n",
      "Epoch 35/100\n",
      "612/612 [==============================] - 42s 68ms/step - loss: 0.5445 - sparse_categorical_accuracy: 0.7188\n",
      "Epoch 36/100\n",
      "612/612 [==============================] - 41s 67ms/step - loss: 0.5448 - sparse_categorical_accuracy: 0.7187\n",
      "Epoch 37/100\n",
      "612/612 [==============================] - 41s 66ms/step - loss: 0.5448 - sparse_categorical_accuracy: 0.7183\n",
      "Epoch 38/100\n",
      "612/612 [==============================] - 40s 65ms/step - loss: 0.5447 - sparse_categorical_accuracy: 0.7190\n",
      "Epoch 39/100\n",
      "612/612 [==============================] - 40s 66ms/step - loss: 0.5442 - sparse_categorical_accuracy: 0.7186\n",
      "Epoch 40/100\n",
      "612/612 [==============================] - 41s 66ms/step - loss: 0.5445 - sparse_categorical_accuracy: 0.7187\n",
      "Epoch 41/100\n",
      "612/612 [==============================] - 40s 66ms/step - loss: 0.5447 - sparse_categorical_accuracy: 0.7186\n",
      "Epoch 42/100\n",
      "612/612 [==============================] - 40s 66ms/step - loss: 0.5444 - sparse_categorical_accuracy: 0.7182\n",
      "Epoch 43/100\n",
      "612/612 [==============================] - 40s 66ms/step - loss: 0.5442 - sparse_categorical_accuracy: 0.7180\n",
      "Epoch 44/100\n",
      "612/612 [==============================] - 40s 66ms/step - loss: 0.5446 - sparse_categorical_accuracy: 0.7188\n",
      "Epoch 45/100\n",
      "612/612 [==============================] - 41s 67ms/step - loss: 0.5445 - sparse_categorical_accuracy: 0.7175\n",
      "Epoch 46/100\n",
      "612/612 [==============================] - 41s 67ms/step - loss: 0.5439 - sparse_categorical_accuracy: 0.7183\n",
      "Epoch 47/100\n",
      "612/612 [==============================] - 41s 67ms/step - loss: 0.5447 - sparse_categorical_accuracy: 0.7181\n",
      "Epoch 48/100\n",
      "612/612 [==============================] - 41s 67ms/step - loss: 0.5446 - sparse_categorical_accuracy: 0.7196\n",
      "Epoch 49/100\n",
      "612/612 [==============================] - 41s 67ms/step - loss: 0.5448 - sparse_categorical_accuracy: 0.7181\n",
      "Epoch 50/100\n",
      "612/612 [==============================] - 40s 66ms/step - loss: 0.5446 - sparse_categorical_accuracy: 0.7190\n",
      "Epoch 51/100\n",
      "612/612 [==============================] - 41s 66ms/step - loss: 0.5445 - sparse_categorical_accuracy: 0.7190\n",
      "Epoch 52/100\n",
      "612/612 [==============================] - 40s 66ms/step - loss: 0.5445 - sparse_categorical_accuracy: 0.7180\n",
      "Epoch 53/100\n",
      "612/612 [==============================] - 41s 67ms/step - loss: 0.5445 - sparse_categorical_accuracy: 0.7178\n",
      "Epoch 54/100\n",
      "612/612 [==============================] - 41s 67ms/step - loss: 0.5448 - sparse_categorical_accuracy: 0.7186\n",
      "Epoch 55/100\n",
      "612/612 [==============================] - 41s 66ms/step - loss: 0.5443 - sparse_categorical_accuracy: 0.7191\n",
      "Epoch 56/100\n",
      "612/612 [==============================] - 41s 66ms/step - loss: 0.5446 - sparse_categorical_accuracy: 0.7191\n",
      "Epoch 57/100\n",
      "612/612 [==============================] - 41s 67ms/step - loss: 0.5445 - sparse_categorical_accuracy: 0.7185\n",
      "Epoch 58/100\n",
      "612/612 [==============================] - 41s 67ms/step - loss: 0.5448 - sparse_categorical_accuracy: 0.7183\n",
      "Epoch 59/100\n",
      "612/612 [==============================] - 41s 66ms/step - loss: 0.5443 - sparse_categorical_accuracy: 0.7191\n",
      "Epoch 60/100\n",
      "612/612 [==============================] - 40s 66ms/step - loss: 0.5445 - sparse_categorical_accuracy: 0.7185\n",
      "Epoch 61/100\n",
      "612/612 [==============================] - 41s 67ms/step - loss: 0.5447 - sparse_categorical_accuracy: 0.7188\n",
      "Epoch 62/100\n",
      "612/612 [==============================] - 41s 67ms/step - loss: 0.5445 - sparse_categorical_accuracy: 0.7180\n",
      "Epoch 63/100\n",
      "612/612 [==============================] - 41s 66ms/step - loss: 0.5445 - sparse_categorical_accuracy: 0.7182\n",
      "Epoch 64/100\n",
      "612/612 [==============================] - 41s 67ms/step - loss: 0.5445 - sparse_categorical_accuracy: 0.7174\n",
      "Epoch 65/100\n",
      "612/612 [==============================] - 41s 67ms/step - loss: 0.5448 - sparse_categorical_accuracy: 0.7179\n",
      "Epoch 66/100\n",
      "612/612 [==============================] - 41s 67ms/step - loss: 0.5443 - sparse_categorical_accuracy: 0.7190\n",
      "Epoch 67/100\n",
      "612/612 [==============================] - 41s 66ms/step - loss: 0.5445 - sparse_categorical_accuracy: 0.7190\n",
      "Epoch 68/100\n",
      "612/612 [==============================] - 41s 66ms/step - loss: 0.5444 - sparse_categorical_accuracy: 0.7190\n",
      "Epoch 69/100\n",
      "612/612 [==============================] - 41s 66ms/step - loss: 0.5444 - sparse_categorical_accuracy: 0.7191\n",
      "Epoch 70/100\n",
      "612/612 [==============================] - 40s 66ms/step - loss: 0.5445 - sparse_categorical_accuracy: 0.7185\n",
      "Epoch 71/100\n",
      "612/612 [==============================] - 41s 67ms/step - loss: 0.5443 - sparse_categorical_accuracy: 0.7193\n",
      "Epoch 72/100\n",
      "612/612 [==============================] - 41s 67ms/step - loss: 0.5445 - sparse_categorical_accuracy: 0.7186\n",
      "Epoch 73/100\n",
      "612/612 [==============================] - 41s 67ms/step - loss: 0.5446 - sparse_categorical_accuracy: 0.7190\n",
      "Epoch 74/100\n",
      "612/612 [==============================] - 41s 67ms/step - loss: 0.5447 - sparse_categorical_accuracy: 0.7190\n",
      "Epoch 75/100\n",
      "612/612 [==============================] - 41s 67ms/step - loss: 0.5446 - sparse_categorical_accuracy: 0.7187\n",
      "Epoch 76/100\n",
      "612/612 [==============================] - 41s 66ms/step - loss: 0.5444 - sparse_categorical_accuracy: 0.7187\n",
      "Epoch 77/100\n",
      "612/612 [==============================] - 41s 66ms/step - loss: 0.5444 - sparse_categorical_accuracy: 0.7180\n",
      "Epoch 78/100\n",
      "612/612 [==============================] - 41s 67ms/step - loss: 0.5447 - sparse_categorical_accuracy: 0.7183\n",
      "Epoch 79/100\n",
      "612/612 [==============================] - 41s 67ms/step - loss: 0.5443 - sparse_categorical_accuracy: 0.7192\n",
      "Epoch 80/100\n",
      "612/612 [==============================] - 41s 67ms/step - loss: 0.5441 - sparse_categorical_accuracy: 0.7190\n",
      "Epoch 81/100\n",
      "612/612 [==============================] - 40s 66ms/step - loss: 0.5446 - sparse_categorical_accuracy: 0.7181\n",
      "Epoch 82/100\n",
      "612/612 [==============================] - 40s 66ms/step - loss: 0.5447 - sparse_categorical_accuracy: 0.7181\n",
      "Epoch 83/100\n",
      "612/612 [==============================] - 40s 66ms/step - loss: 0.5443 - sparse_categorical_accuracy: 0.7186\n",
      "Epoch 84/100\n",
      "612/612 [==============================] - 40s 66ms/step - loss: 0.5445 - sparse_categorical_accuracy: 0.7190\n",
      "Epoch 85/100\n",
      "612/612 [==============================] - 40s 65ms/step - loss: 0.5447 - sparse_categorical_accuracy: 0.7189\n",
      "Epoch 86/100\n",
      "612/612 [==============================] - 40s 65ms/step - loss: 0.5443 - sparse_categorical_accuracy: 0.7182\n",
      "Epoch 87/100\n",
      "612/612 [==============================] - 40s 66ms/step - loss: 0.5444 - sparse_categorical_accuracy: 0.7183\n",
      "Epoch 88/100\n",
      "612/612 [==============================] - 40s 65ms/step - loss: 0.5445 - sparse_categorical_accuracy: 0.7189\n",
      "Epoch 89/100\n",
      "612/612 [==============================] - 70s 114ms/step - loss: 0.5445 - sparse_categorical_accuracy: 0.7181\n",
      "Epoch 90/100\n",
      "612/612 [==============================] - 40s 66ms/step - loss: 0.5445 - sparse_categorical_accuracy: 0.7187\n",
      "Epoch 91/100\n",
      "612/612 [==============================] - 339s 555ms/step - loss: 0.5444 - sparse_categorical_accuracy: 0.7192\n",
      "Epoch 92/100\n",
      "612/612 [==============================] - 236s 386ms/step - loss: 0.5443 - sparse_categorical_accuracy: 0.7181\n",
      "Epoch 93/100\n",
      "612/612 [==============================] - 40s 66ms/step - loss: 0.5445 - sparse_categorical_accuracy: 0.7193\n",
      "Epoch 94/100\n",
      "612/612 [==============================] - 42s 69ms/step - loss: 0.5445 - sparse_categorical_accuracy: 0.7197\n",
      "Epoch 95/100\n",
      "612/612 [==============================] - 45s 73ms/step - loss: 0.5445 - sparse_categorical_accuracy: 0.7188\n",
      "Epoch 96/100\n",
      "612/612 [==============================] - 44s 72ms/step - loss: 0.5444 - sparse_categorical_accuracy: 0.7185\n",
      "Epoch 97/100\n",
      "612/612 [==============================] - 44s 73ms/step - loss: 0.5444 - sparse_categorical_accuracy: 0.7176\n",
      "Epoch 98/100\n",
      "612/612 [==============================] - 44s 72ms/step - loss: 0.5444 - sparse_categorical_accuracy: 0.7182\n",
      "Epoch 99/100\n",
      "612/612 [==============================] - 44s 72ms/step - loss: 0.5443 - sparse_categorical_accuracy: 0.7190\n",
      "Epoch 100/100\n",
      "612/612 [==============================] - 44s 72ms/step - loss: 0.5442 - sparse_categorical_accuracy: 0.7186\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x2a0963d50>"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(\n",
    "    X_train,\n",
    "    Y_train,\n",
    "    epochs=100,\n",
    "    batch_size=64,\n",
    "    # callbacks=callbacks,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c84f1173",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"transformer_classifier.keras\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "a39f33ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1224/1224 [==============================] - 22s 18ms/step\n"
     ]
    }
   ],
   "source": [
    "y_hat_train = model.predict(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "92b2bdc2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18598"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(y_hat_train.argmax(axis=1) == 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e43dfd6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
